{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Принцип работы этой модели:\n",
    "\n",
    "Идея FastText в том, что морфология важна для определения значения слова. Если в Word2Vec для каждого слова создается отдельный вектор, то в FastText слово представлено как сумма векторов его компонентов (char-нграмм). Таким образом векторы могут быть сформированы даже для out-of-vocabulary слов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training models\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-25 19:23:01,677 : INFO : collecting all words and their counts\n",
      "2024-01-25 19:23:01,678 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2024-01-25 19:23:01,699 : INFO : PROGRESS: at sentence #10000, processed 48251 words, keeping 9798 word types\n",
      "2024-01-25 19:23:01,721 : INFO : PROGRESS: at sentence #20000, processed 102675 words, keeping 16517 word types\n",
      "2024-01-25 19:23:01,744 : INFO : PROGRESS: at sentence #30000, processed 155232 words, keeping 21473 word types\n",
      "2024-01-25 19:23:01,765 : INFO : collected 25670 word types from a corpus of 202651 raw words and 40000 sentences\n",
      "2024-01-25 19:23:01,766 : INFO : Creating a fresh vocabulary\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['First', 'Citizen:'],\n",
      " ['Before', 'we', 'proceed', 'any', 'further,', 'hear', 'me', 'speak.'],\n",
      " [],\n",
      " ['All:'],\n",
      " ['Speak,', 'speak.'],\n",
      " [],\n",
      " ['First', 'Citizen:'],\n",
      " ['You',\n",
      "  'are',\n",
      "  'all',\n",
      "  'resolved',\n",
      "  'rather',\n",
      "  'to',\n",
      "  'die',\n",
      "  'than',\n",
      "  'to',\n",
      "  'famish?'],\n",
      " [],\n",
      " ['All:'],\n",
      " ['Resolved.', 'resolved.'],\n",
      " [],\n",
      " ['First', 'Citizen:'],\n",
      " ['First,',\n",
      "  'you',\n",
      "  'know',\n",
      "  'Caius',\n",
      "  'Marcius',\n",
      "  'is',\n",
      "  'chief',\n",
      "  'enemy',\n",
      "  'to',\n",
      "  'the',\n",
      "  'people.'],\n",
      " [],\n",
      " ['All:'],\n",
      " ['We', \"know't,\", 'we', \"know't.\"],\n",
      " [],\n",
      " ['First', 'Citizen:'],\n",
      " ['Let',\n",
      "  'us',\n",
      "  'kill',\n",
      "  'him,',\n",
      "  'and',\n",
      "  \"we'll\",\n",
      "  'have',\n",
      "  'corn',\n",
      "  'at',\n",
      "  'our',\n",
      "  'own',\n",
      "  'price.'],\n",
      " [\"Is't\", 'a', 'verdict?'],\n",
      " [],\n",
      " ['All:'],\n",
      " ['No',\n",
      "  'more',\n",
      "  'talking',\n",
      "  \"on't;\",\n",
      "  'let',\n",
      "  'it',\n",
      "  'be',\n",
      "  'done:',\n",
      "  'away,',\n",
      "  'away!'],\n",
      " [],\n",
      " ['Second', 'Citizen:'],\n",
      " ['One', 'word,', 'good', 'citizens.'],\n",
      " [],\n",
      " ['First', 'Citizen:'],\n",
      " ['We', 'are', 'accounted', 'poor', 'citizens,', 'the', 'patricians', 'good.'],\n",
      " ['What',\n",
      "  'authority',\n",
      "  'surfeits',\n",
      "  'on',\n",
      "  'would',\n",
      "  'relieve',\n",
      "  'us:',\n",
      "  'if',\n",
      "  'they'],\n",
      " ['would', 'yield', 'us', 'but', 'the', 'superfluity,', 'while', 'it', 'were'],\n",
      " ['wholesome,', 'we', 'might', 'guess', 'they', 'relieved', 'us', 'humanely;'],\n",
      " ['but',\n",
      "  'they',\n",
      "  'think',\n",
      "  'we',\n",
      "  'are',\n",
      "  'too',\n",
      "  'dear:',\n",
      "  'the',\n",
      "  'leanness',\n",
      "  'that'],\n",
      " ['afflicts', 'us,', 'the', 'object', 'of', 'our', 'misery,', 'is', 'as', 'an'],\n",
      " ['inventory', 'to', 'particularise', 'their', 'abundance;', 'our'],\n",
      " ['sufferance',\n",
      "  'is',\n",
      "  'a',\n",
      "  'gain',\n",
      "  'to',\n",
      "  'them',\n",
      "  'Let',\n",
      "  'us',\n",
      "  'revenge',\n",
      "  'this',\n",
      "  'with'],\n",
      " ['our',\n",
      "  'pikes,',\n",
      "  'ere',\n",
      "  'we',\n",
      "  'become',\n",
      "  'rakes:',\n",
      "  'for',\n",
      "  'the',\n",
      "  'gods',\n",
      "  'know',\n",
      "  'I'],\n",
      " ['speak',\n",
      "  'this',\n",
      "  'in',\n",
      "  'hunger',\n",
      "  'for',\n",
      "  'bread,',\n",
      "  'not',\n",
      "  'in',\n",
      "  'thirst',\n",
      "  'for',\n",
      "  'revenge.'],\n",
      " [],\n",
      " ['Second', 'Citizen:'],\n",
      " ['Would', 'you', 'proceed', 'especially', 'against', 'Caius', 'Marcius?'],\n",
      " [],\n",
      " ['All:'],\n",
      " ['Against',\n",
      "  'him',\n",
      "  'first:',\n",
      "  \"he's\",\n",
      "  'a',\n",
      "  'very',\n",
      "  'dog',\n",
      "  'to',\n",
      "  'the',\n",
      "  'commonalty.'],\n",
      " [],\n",
      " ['Second', 'Citizen:'],\n",
      " ['Consider',\n",
      "  'you',\n",
      "  'what',\n",
      "  'services',\n",
      "  'he',\n",
      "  'has',\n",
      "  'done',\n",
      "  'for',\n",
      "  'his',\n",
      "  'country?'],\n",
      " [],\n",
      " ['First', 'Citizen:'],\n",
      " ['Very',\n",
      "  'well;',\n",
      "  'and',\n",
      "  'could',\n",
      "  'be',\n",
      "  'content',\n",
      "  'to',\n",
      "  'give',\n",
      "  'him',\n",
      "  'good'],\n",
      " ['report',\n",
      "  'fort,',\n",
      "  'but',\n",
      "  'that',\n",
      "  'he',\n",
      "  'pays',\n",
      "  'himself',\n",
      "  'with',\n",
      "  'being',\n",
      "  'proud.'],\n",
      " [],\n",
      " ['Second', 'Citizen:'],\n",
      " ['Nay,', 'but', 'speak', 'not', 'maliciously.'],\n",
      " [],\n",
      " ['First', 'Citizen:'],\n",
      " ['I',\n",
      "  'say',\n",
      "  'unto',\n",
      "  'you,',\n",
      "  'what',\n",
      "  'he',\n",
      "  'hath',\n",
      "  'done',\n",
      "  'famously,',\n",
      "  'he',\n",
      "  'did'],\n",
      " ['it', 'to', 'that', 'end:', 'though', 'soft-conscienced', 'men', 'can', 'be'],\n",
      " ['content',\n",
      "  'to',\n",
      "  'say',\n",
      "  'it',\n",
      "  'was',\n",
      "  'for',\n",
      "  'his',\n",
      "  'country',\n",
      "  'he',\n",
      "  'did',\n",
      "  'it',\n",
      "  'to'],\n",
      " ['please',\n",
      "  'his',\n",
      "  'mother',\n",
      "  'and',\n",
      "  'to',\n",
      "  'be',\n",
      "  'partly',\n",
      "  'proud;',\n",
      "  'which',\n",
      "  'he'],\n",
      " ['is,', 'even', 'till', 'the', 'altitude', 'of', 'his', 'virtue.'],\n",
      " [],\n",
      " ['Second', 'Citizen:'],\n",
      " ['What',\n",
      "  'he',\n",
      "  'cannot',\n",
      "  'help',\n",
      "  'in',\n",
      "  'his',\n",
      "  'nature,',\n",
      "  'you',\n",
      "  'account',\n",
      "  'a'],\n",
      " ['vice',\n",
      "  'in',\n",
      "  'him.',\n",
      "  'You',\n",
      "  'must',\n",
      "  'in',\n",
      "  'no',\n",
      "  'way',\n",
      "  'say',\n",
      "  'he',\n",
      "  'is',\n",
      "  'covetous.'],\n",
      " [],\n",
      " ['First', 'Citizen:'],\n",
      " ['If',\n",
      "  'I',\n",
      "  'must',\n",
      "  'not,',\n",
      "  'I',\n",
      "  'need',\n",
      "  'not',\n",
      "  'be',\n",
      "  'barren',\n",
      "  'of',\n",
      "  'accusations;'],\n",
      " ['he',\n",
      "  'hath',\n",
      "  'faults,',\n",
      "  'with',\n",
      "  'surplus,',\n",
      "  'to',\n",
      "  'tire',\n",
      "  'in',\n",
      "  'repetition.'],\n",
      " ['What',\n",
      "  'shouts',\n",
      "  'are',\n",
      "  'these?',\n",
      "  'The',\n",
      "  'other',\n",
      "  'side',\n",
      "  \"o'\",\n",
      "  'the',\n",
      "  'city'],\n",
      " ['is',\n",
      "  'risen:',\n",
      "  'why',\n",
      "  'stay',\n",
      "  'we',\n",
      "  'prating',\n",
      "  'here?',\n",
      "  'to',\n",
      "  'the',\n",
      "  'Capitol!'],\n",
      " [],\n",
      " ['All:'],\n",
      " ['Come,', 'come.'],\n",
      " [],\n",
      " ['First', 'Citizen:'],\n",
      " ['Soft!', 'who', 'comes', 'here?'],\n",
      " [],\n",
      " ['Second', 'Citizen:'],\n",
      " ['Worthy', 'Menenius', 'Agrippa;', 'one', 'that', 'hath', 'always', 'loved'],\n",
      " ['the', 'people.'],\n",
      " [],\n",
      " ['First', 'Citizen:'],\n",
      " [\"He's\",\n",
      "  'one',\n",
      "  'honest',\n",
      "  'enough:',\n",
      "  'would',\n",
      "  'all',\n",
      "  'the',\n",
      "  'rest',\n",
      "  'were',\n",
      "  'so!'],\n",
      " [],\n",
      " ['MENENIUS:'],\n",
      " ['What', \"work's,\", 'my', 'countrymen,', 'in', 'hand?', 'where', 'go', 'you'],\n",
      " ['With',\n",
      "  'bats',\n",
      "  'and',\n",
      "  'clubs?',\n",
      "  'The',\n",
      "  'matter?',\n",
      "  'speak,',\n",
      "  'I',\n",
      "  'pray',\n",
      "  'you.'],\n",
      " [],\n",
      " ['First', 'Citizen:'],\n",
      " ['Our',\n",
      "  'business',\n",
      "  'is',\n",
      "  'not',\n",
      "  'unknown',\n",
      "  'to',\n",
      "  'the',\n",
      "  'senate;',\n",
      "  'they',\n",
      "  'have'],\n",
      " ['had', 'inkling', 'this', 'fortnight', 'what', 'we', 'intend', 'to', 'do,'],\n",
      " ['which',\n",
      "  'now',\n",
      "  \"we'll\",\n",
      "  'show',\n",
      "  \"'em\",\n",
      "  'in',\n",
      "  'deeds.',\n",
      "  'They',\n",
      "  'say',\n",
      "  'poor'],\n",
      " ['suitors', 'have', 'strong', 'breaths:', 'they', 'shall', 'know', 'we'],\n",
      " ['have', 'strong', 'arms', 'too.'],\n",
      " [],\n",
      " ['MENENIUS:'],\n",
      " ['Why,',\n",
      "  'masters,',\n",
      "  'my',\n",
      "  'good',\n",
      "  'friends,',\n",
      "  'mine',\n",
      "  'honest',\n",
      "  'neighbours,'],\n",
      " ['Will', 'you', 'undo', 'yourselves?']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-25 19:23:01,869 : INFO : FastText lifecycle event {'msg': 'effective_min_count=1 retains 25670 unique words (100.00% of original 25670, drops 0)', 'datetime': '2024-01-25T19:23:01.869391', 'gensim': '4.3.2', 'python': '3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'prepare_vocab'}\n",
      "2024-01-25 19:23:01,870 : INFO : FastText lifecycle event {'msg': 'effective_min_count=1 leaves 202651 word corpus (100.00% of original 202651, drops 0)', 'datetime': '2024-01-25T19:23:01.870350', 'gensim': '4.3.2', 'python': '3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'prepare_vocab'}\n",
      "2024-01-25 19:23:02,034 : INFO : deleting the raw counts dictionary of 25670 items\n",
      "2024-01-25 19:23:02,035 : INFO : sample=0.001 downsamples 45 most-common words\n",
      "2024-01-25 19:23:02,037 : INFO : FastText lifecycle event {'msg': 'downsampling leaves estimated 168646.91294783162 word corpus (83.2%% of prior 202651)', 'datetime': '2024-01-25T19:23:02.037522', 'gensim': '4.3.2', 'python': '3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'prepare_vocab'}\n",
      "2024-01-25 19:23:02,444 : INFO : estimated required memory for 25670 words, 2000000 buckets and 100 dimensions: 838234008 bytes\n",
      "2024-01-25 19:23:02,444 : INFO : resetting layer weights\n",
      "2024-01-25 19:23:05,523 : INFO : FastText lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-01-25T19:23:05.523582', 'gensim': '4.3.2', 'python': '3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'build_vocab'}\n",
      "2024-01-25 19:23:05,524 : INFO : FastText lifecycle event {'msg': 'training model with 4 workers on 25670 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2024-01-25T19:23:05.524427', 'gensim': '4.3.2', 'python': '3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'train'}\n",
      "2024-01-25 19:23:05,926 : INFO : EPOCH 0: training on 202651 raw words (168501 effective words) took 0.4s, 448115 effective words/s\n",
      "2024-01-25 19:23:06,327 : INFO : EPOCH 1: training on 202651 raw words (168769 effective words) took 0.4s, 440931 effective words/s\n",
      "2024-01-25 19:23:06,727 : INFO : EPOCH 2: training on 202651 raw words (168537 effective words) took 0.4s, 448473 effective words/s\n",
      "2024-01-25 19:23:07,120 : INFO : EPOCH 3: training on 202651 raw words (168423 effective words) took 0.4s, 450302 effective words/s\n",
      "2024-01-25 19:23:07,526 : INFO : EPOCH 4: training on 202651 raw words (168457 effective words) took 0.4s, 436615 effective words/s\n",
      "2024-01-25 19:23:07,527 : INFO : FastText lifecycle event {'msg': 'training on 1013255 raw words (842687 effective words) took 2.0s, 420928 effective words/s', 'datetime': '2024-01-25T19:23:07.527274', 'gensim': '4.3.2', 'python': '3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'train'}\n",
      "2024-01-25 19:23:08,713 : INFO : FastText lifecycle event {'params': 'FastText<vocab=25670, vector_size=100, alpha=0.025>', 'datetime': '2024-01-25T19:23:08.713740', 'gensim': '4.3.2', 'python': '3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'created'}\n",
      "2024-01-25 19:23:08,807 : INFO : collecting all words and their counts\n",
      "2024-01-25 19:23:08,809 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2024-01-25 19:23:08,833 : INFO : collected 10781 word types from a corpus of 59890 raw words and 300 sentences\n",
      "2024-01-25 19:23:08,833 : INFO : Creating a fresh vocabulary\n",
      "2024-01-25 19:23:08,846 : INFO : FastText lifecycle event {'msg': 'effective_min_count=5 retains 1762 unique words (16.34% of original 10781, drops 9019)', 'datetime': '2024-01-25T19:23:08.846942', 'gensim': '4.3.2', 'python': '3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'prepare_vocab'}\n",
      "2024-01-25 19:23:08,847 : INFO : FastText lifecycle event {'msg': 'effective_min_count=5 leaves 46084 word corpus (76.95% of original 59890, drops 13806)', 'datetime': '2024-01-25T19:23:08.847973', 'gensim': '4.3.2', 'python': '3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'prepare_vocab'}\n",
      "2024-01-25 19:23:08,862 : INFO : deleting the raw counts dictionary of 10781 items\n",
      "2024-01-25 19:23:08,862 : INFO : sample=0.001 downsamples 45 most-common words\n",
      "2024-01-25 19:23:08,863 : INFO : FastText lifecycle event {'msg': 'downsampling leaves estimated 32610.61883565215 word corpus (70.8%% of prior 46084)', 'datetime': '2024-01-25T19:23:08.863931', 'gensim': '4.3.2', 'python': '3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'prepare_vocab'}\n",
      "2024-01-25 19:23:08,865 : WARNING : sorting after vectors have been allocated is expensive & error-prone\n",
      "2024-01-25 19:23:08,912 : INFO : estimated required memory for 1762 words, 2000000 buckets and 100 dimensions: 802597824 bytes\n",
      "2024-01-25 19:23:08,913 : INFO : resetting layer weights\n",
      "2024-01-25 19:23:09,024 : INFO : FastText lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-01-25T19:23:09.024222', 'gensim': '4.3.2', 'python': '3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'build_vocab'}\n",
      "2024-01-25 19:23:09,025 : WARNING : Effective 'alpha' higher than previous training cycles\n",
      "2024-01-25 19:23:09,026 : INFO : FastText lifecycle event {'msg': 'training model with 4 workers on 25670 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2024-01-25T19:23:09.026175', 'gensim': '4.3.2', 'python': '3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'train'}\n",
      "2024-01-25 19:23:09,182 : INFO : EPOCH 0: training on 75508 raw words (37472 effective words) took 0.1s, 320375 effective words/s\n",
      "2024-01-25 19:23:09,339 : INFO : EPOCH 1: training on 75508 raw words (37319 effective words) took 0.1s, 306730 effective words/s\n",
      "2024-01-25 19:23:09,491 : INFO : EPOCH 2: training on 75508 raw words (37399 effective words) took 0.1s, 320433 effective words/s\n",
      "2024-01-25 19:23:09,643 : INFO : EPOCH 3: training on 75508 raw words (37441 effective words) took 0.1s, 330731 effective words/s\n",
      "2024-01-25 19:23:09,803 : INFO : EPOCH 4: training on 75508 raw words (37438 effective words) took 0.1s, 307524 effective words/s\n",
      "2024-01-25 19:23:09,807 : INFO : FastText lifecycle event {'msg': 'training on 377540 raw words (187069 effective words) took 0.8s, 239682 effective words/s', 'datetime': '2024-01-25T19:23:09.807415', 'gensim': '4.3.2', 'python': '3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'train'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<gensim.models.fasttext.FastText object at 0x000002760F697760>\n"
     ]
    }
   ],
   "source": [
    "#!pip3 install gensim --user\n",
    "\n",
    "from pprint import pprint as print\n",
    "from gensim.models.fasttext import FastText\n",
    "from gensim.test.utils import datapath\n",
    "import re\n",
    "\n",
    "# функция для считывания и загрузки файла\n",
    "def read_sentences_from_file(file_path):\n",
    "    # открываем файл\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        # здесь может быть какая угодно другая предобработка текста\n",
    "        sentences = [line.strip().split() for line in file]\n",
    "        #print(\"here \", type(sentences))\n",
    "        #for i in range(len(sentences)):\n",
    "        #    sentences[i] = str(sentences[i]).lower()\n",
    "        #    sentences[i] = re.sub(r'[^a-z\\s]', '', str(sentences[i]))\n",
    "            #sentences[i] = re.sub(r'\\s+', ' ', sentences[i]).strip()\n",
    "    return sentences\n",
    "\n",
    "custom_sentences = read_sentences_from_file('input.txt')\n",
    "print(custom_sentences[:100]) # [['First', 'Citizen:'] ...\n",
    "\n",
    "# обучаем модель, прописываем параметры, какие нам нравятся, все по аналогии с Word2Vec\n",
    "custom_model = FastText(sentences=custom_sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# build the vocabulary\n",
    "model.build_vocab(corpus_file=corpus_file)\n",
    "\n",
    "# train the model\n",
    "custom_model.train(\n",
    "    corpus_file=corpus_file, epochs=model.epochs,\n",
    "    total_examples=model.corpus_count, total_words=model.corpus_total_words,\n",
    ")\n",
    "\n",
    "print(custom_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word vector lookup\n",
    "\n",
    "\n",
    "All information necessary for looking up fastText words (incl. OOV words) is\n",
    "contained in its ``model.wv`` attribute.\n",
    "\n",
    "If you don't need to continue training your model, you can export & save this `.wv`\n",
    "attribute and discard `model`, to save space and RAM.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<gensim.models.fasttext.FastTextKeyedVectors object at 0x000002760EE04160>\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "wv = custom_model.wv\n",
    "print(wv)\n",
    "\n",
    "#\n",
    "# FastText models support vector lookups for out-of-vocabulary words by summing up character ngrams belonging to the word.\n",
    "#\n",
    "print('first' in wv.key_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print('queen' in wv.key_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([ 0.17036636,  0.11650486, -1.0494837 , -0.06330542,  0.58523583,\n",
      "        0.50222754,  0.3042531 ,  1.1636422 , -0.40916002, -0.56979954,\n",
      "        0.5112044 , -0.28100216,  0.02548225,  1.4353436 , -0.70828426,\n",
      "       -0.80785763, -0.04859999, -0.03423959, -0.7232618 , -0.56549174,\n",
      "       -0.5198323 ,  0.47164232, -0.5048996 , -0.2741103 , -0.16621596,\n",
      "       -0.488023  , -0.77342886, -0.5510966 ,  0.17865784,  0.9906498 ,\n",
      "       -0.7956326 ,  0.11637522,  1.1931915 , -0.56495327, -0.17591673,\n",
      "        0.3511306 , -0.69072187, -0.33180344, -0.6337029 , -0.30346203,\n",
      "        0.52230066, -0.5429514 ,  0.39075312, -0.27921364,  0.23890541,\n",
      "       -0.3825281 , -0.05146267,  0.05710865,  0.6314949 ,  0.15411547,\n",
      "        0.18805647, -0.2885942 ,  0.23656331, -1.8217834 , -0.01878889,\n",
      "       -0.55638427, -0.6235669 , -0.02545961, -0.13432382,  0.1546252 ,\n",
      "       -0.28258798, -0.5002011 , -0.04425686,  1.1610763 , -0.05549501,\n",
      "        1.3503082 , -0.20217952, -0.00494334,  0.6147996 ,  0.51056015,\n",
      "       -0.39072663,  0.9082786 ,  0.32591546, -0.9466181 ,  0.5009395 ,\n",
      "       -0.01604007,  0.75834274,  0.40839568, -0.5478016 ,  0.68419147,\n",
      "       -0.55139   , -1.1786153 , -1.4326452 , -0.02797619,  0.10037483,\n",
      "       -0.8518488 ,  0.720767  ,  0.25922108, -0.28143084, -0.12727405,\n",
      "        0.3850354 , -0.12376192, -0.10566024,  0.04078031, -0.46448085,\n",
      "        0.3507937 ,  0.32428896, -0.9271769 ,  0.509128  ,  0.5707744 ],\n",
      "      dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(wv['queen'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity operations\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9934779\n"
     ]
    }
   ],
   "source": [
    "print(wv.similarity(\"queen\", \"king\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99822974\n"
     ]
    }
   ],
   "source": [
    "print(wv.similarity(\"first\", \"second\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other similarity operations\n",
    "\n",
    "Вот тут отлично видно, что модель ориентируется на последовательность символов\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('liking', 0.9999139904975891),\n",
      " ('king?', 0.9998611211776733),\n",
      " (\"king'?\", 0.9998340010643005),\n",
      " ('Drinking', 0.9998327493667603),\n",
      " ('king:', 0.9998314380645752),\n",
      " ('Doing', 0.9998264908790588),\n",
      " ('Wooing', 0.9998261332511902),\n",
      " ('seeking', 0.999819815158844),\n",
      " ('asking', 0.9998108744621277),\n",
      " ('king;', 0.9997879266738892)]\n"
     ]
    }
   ],
   "source": [
    "print(wv.most_similar(\"king\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'king'\n"
     ]
    }
   ],
   "source": [
    "print(wv.doesnt_match(\"queen king knight\".split()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
